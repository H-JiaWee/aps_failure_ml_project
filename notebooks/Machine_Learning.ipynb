{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "from functions import *\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training set is  (60000, 171)\n",
      "shape of test set is  (16000, 171)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/raw/aps_failure_training_set.csv\")\n",
    "test = pd.read_csv(\"../data/raw/aps_failure_test_set.csv\")\n",
    "\n",
    "print('shape of training set is ',train.shape)\n",
    "print('shape of test set is ',test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. Replace 'na' with null value and convert column into numeric data type\n",
    "2. Imputing missing variables with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###defining target variables(y) and predictor variables (x)\n",
    "y_train = train['class']\n",
    "X_train = train.drop('class',axis=1)\n",
    "\n",
    "y_test = test['class']\n",
    "X_test = test.drop('class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace 'na' with null value and convert column into numeric data type\n",
    "\n",
    "X_train.replace('na',np.nan,inplace=True)\n",
    "X_train=X_train[X_train.columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "X_test.replace('na',np.nan,inplace=True)\n",
    "X_test=X_test[X_train.columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imputing mising variables\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X_train_new_adjusted= pd.DataFrame(imp.fit_transform(X_train),columns=X_train.columns)\n",
    "X_test_new_adjusted= pd.DataFrame(imp.fit_transform(X_test),columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new_adjusted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Function testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess(train)\n",
    "X_test = preprocess(test)\n",
    "\n",
    "y_train = train['class']\n",
    "y_test = test['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The super fancy but copy-pastable machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training/Fitting model using training data\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using model to generate prediction using test data predictor variables\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred=pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compute test dataset cost\n",
    "\n",
    "cost(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JW's section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "smote = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote, y_train_smote = balance_data(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smote Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hengj\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrongly labelled positives = 249\n",
      "wrongly labelled negatives = 41\n",
      "total cost of wrongly labelling = 22990\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=99999)\n",
    "clf.fit(X_train_smote,y_train_smote)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "cost(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smote Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrongly labelled positives = 766\n",
      "wrongly labelled negatives = 88\n",
      "total cost of wrongly labelling = 51660\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train_smote,y_train_smote)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "cost(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrongly labelled positives = 309\n",
      "wrongly labelled negatives = 39\n",
      "total cost of wrongly labelling = 22590\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.fit(X_train_smote,y_train_smote)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "cost(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrongly labelled positives = 371\n",
      "wrongly labelled negatives = 31\n",
      "total cost of wrongly labelling = 19210\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train_smote,y_train_smote)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "cost(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrongly labelled positives = 181\n",
      "wrongly labelled negatives = 44\n",
      "total cost of wrongly labelling = 23810\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train_smote,y_train_smote)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "cost(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d51f260eded27f650258a4de4708081dac957813be8f781887b52b18819514a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
